# E-Commerce Customer Segmentation System

[![Python](https://img.shields.io/badge/Python-3.11-blue?style=flat&logo=python)](https://www.python.org/)
[![FastAPI](https://img.shields.io/badge/FastAPI-0.109-009688?style=flat&logo=fastapi)](https://fastapi.tiangolo.com/)
[![Streamlit](https://img.shields.io/badge/Streamlit-1.31-FF4B4B?style=flat&logo=streamlit)](https://streamlit.io/)
[![Docker](https://img.shields.io/badge/Docker-Enabled-2496ED?style=flat&logo=docker)](https://www.docker.com/)
[![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Live_Demo-yellow)](https://huggingface.co/spaces/YOUR_USERNAME/customer-segmentation)

## Project Overview
This project is an End-to-End Machine Learning application designed to analyze customer purchase behavior using **RFM Analysis** (Recency, Frequency, Monetary). It segments customers into distinct groups using **K-Means Clustering**, helping businesses tailor marketing strategies, improve customer retention, and maximize revenue.

The system is fully containerized using **Docker** and deployed as a microservices architecture consisting of a **FastAPI** backend and a **Streamlit** frontend.

 **Live Demo Available Here: ([https://huggingface.co/spaces/YOUR_USERNAME/customer-segmentation](https://huggingface.co/spaces/LluisTaha/customer-segmentation))**

---

## Architecture & Tech Stack

The project follows a decoupled architecture ensuring scalability and maintainability:

* ** Machine Learning:**
    * **RFM Analysis:** Feature engineering based on transaction history.
    * **Preprocessing:** Log transformation & StandardScaler for normalization.
    * **Dimensionality Reduction:** PCA (Principal Component Analysis) to optimize model performance.
    * **Clustering:** K-Means algorithm to identify customer segments.
* **Backend (API):**
    * **FastAPI:** High-performance web framework for serving the model.
    * **Pydantic:** For data validation and schema definition.
* **Frontend (UI):**
    * **Streamlit:** Interactive dashboard for real-time predictions and visualization.
* **DevOps:**
    * **Docker & Docker Compose:** Containerization of services to ensure environment consistency.
    * **Hugging Face Spaces:** Cloud deployment platform.

---

## Methodology (The Science Behind It)

1.  **Data Preprocessing:** Handled missing values and outliers to ensure data quality.
2.  **Feature Engineering (RFM):**
    * **Recency:** Days since the last purchase.
    * **Frequency:** Total number of transactions.
    * **Monetary:** Total revenue generated by the customer.
    * **Unique Products:** Diversity of purchased items.
3.  **Scaling & PCA:** Applied Log Transformation to handle skewness in monetary data, followed by PCA to reduce dimensionality while retaining variance.
4.  **Model Training:** Trained a K-Means clustering model. The optimal number of clusters ($k$) was determined using the Elbow Method and Silhouette Score.

---

## How to Run Locally

You can run the entire application using Docker Compose without worrying about dependencies.

### Prerequisites
* Docker & Docker Compose installed.

### Steps
1.  **Clone the repository:**
    ```bash
    git clone [https://github.com/YOUR_USERNAME/customer-segmentation.git](https://github.com/YOUR_USERNAME/customer-segmentation.git)
    cd customer-segmentation
    ```

2.  **Build and Run:**
    ```bash
    docker compose up --build
    ```

3.  **Access the Application:**
    * **Frontend (Dashboard):** Open `http://localhost:8501`
    * **Backend (API Docs):** Open `http://localhost:8000/docs`

---

## API Usage

The model is exposed via a REST API. You can send a POST request to get the customer segment.

**Endpoint:** `POST /predict`

**Request Body (JSON):**
```json
{
  "Recency": 15,
  "Frequency": 50,
  "Monetary": 1200.5,
  "Unique_Products": 10
}
